import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

# 1. Cargar secretos (Leemos la GROQ_API_KEY del .env)
load_dotenv()

# VerificaciÃ³n de seguridad (Opcional, pero Ãºtil para depurar)
api_key = os.getenv("GROQ_API_KEY")
if not api_key:
    print("âŒ ERROR: No se ha encontrado la GROQ_API_KEY en el archivo .env")
    exit()

def probar_ia():
    print("ğŸ§  Conectando con el cerebro digital (Groq)...")
    
    # 2. Configurar el Modelo
    # Usamos "llama3-8b-8192" porque es rÃ¡pido, gratis y muy capaz.
    chat = ChatGroq(
        temperature=0,             # 0 = Respuestas precisas/tÃ©cnicas (sin alucinaciones)
        model_name="llama-3.1-8b-instant"
    )

    # 3. Definir el "Rol" (Prompt Engineering)
    # AquÃ­ es donde le damos la personalidad de experto en CATIA.
    system_prompt = "Eres un consultor experto en CATIA V5 y dibujo tÃ©cnico. Responde de forma breve y tÃ©cnica."
    human_prompt = "{pregunta_usuario}"

    # Creamos la plantilla que une las dos partes
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", human_prompt),
    ])

    # 4. Crear la Cadena (Chain)
    # LangChain usa el sÃ­mbolo "|" (pipe) para encadenar: Prompt -> Modelo
    chain = prompt | chat

    # 5. Ejecutar la prueba
    consulta = "Tengo un error al hacer un Pad. Me dice que el perfil no estÃ¡ cerrado. Â¿QuÃ© hago?"
    print(f"ğŸ‘¤ Usuario pregunta: {consulta}")
    print("â³ Pensando...")

    try:
        respuesta = chain.invoke({"pregunta_usuario": consulta})
        print("\nğŸ¤– IA Responde:")
        print("-" * 50)
        print(respuesta.content)
        print("-" * 50)
        print("âœ… Â¡Prueba de conexiÃ³n exitosa!")
        
    except Exception as e:
        print(f"âŒ Error al conectar con Groq: {e}")

if __name__ == "__main__":
    probar_ia()